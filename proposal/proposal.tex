% This is borrowed from the 2021 LaTeX2e package for submissions to the Conference on Neural Information Processing Systems (NeurIPS). All credits go to the authors of the original package, which was obtained from this website:
% https://nips.cc/Conferences/2021/PaperInformation/StyleFiles

\documentclass{article}
\usepackage[final, nonatbib]{neurips_2021}
\usepackage[numbers]{natbib}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{amsmath}        
\usepackage{multicol}


\title{Benchmarking context- and gradient-based Meta-Reinforcement Learning}
% You should enter the name of your project here.

\author{
    Patrik Okanovic \\
    \texttt{pokanovic@ethz.ch} \\
    \And
    Rafael Sterzinger \\
    \texttt{rsterzinger@ethz.ch} \\
    \AND
    Fatjon Zogaj \\
    \texttt{fzogaj@ethz.ch} \\
}
% make sure to include the full names of all the team members

%   Project title and the full names of all team members;
    %The problem that you will be investigating and motivation;
    %Related state-of-the-art and potential challenges of this project;
    %The proposed or tentative method/algorithm/implementation to address the underlying problem;
    %Related references that you will examine in later stages (if any);
    %Evaluation plan of your results (if any); 

\begin{document}

\maketitle

\section{Problem Description}
%Describe the problem you will be investigating and explain the motivations. Why do you think this is an interesting problem?
In order to tackle the problem of needing large amounts of data to learn a particular task, we aim at utilizing meta-learning to find a common representation of a group of similar tasks. We analyze, benchmark and compare a variety of meta-reinforcement learning algorithms on multiple tasks, initially trained on some underlying, common task distribution.

\section{Challenges}
%What is the related state-of-the-art work on this topic? What are the potential challenges of this project?
Potential challenges mainly include the overall computational requirements for an exhaustive evaluation as well as gathering training samples for the offline meta-reinforcement learning setting.

As the tasks/environments will also highly differ in their dimensionality, it could be that some algorithms might need more fine-tuning than others or are less robust in certain settings. This further adds to the possible computational burden and the complexity of comparing the algorithms.

Finally, as mentioned in \cite{mitchell_offline_nodate}, we assume that offline algorithms will perform worse than online variants due to the lack of exploration during testing. 
For this, we might have to consider different metrics to compare these two types of algorithms fairly such as performance in relation to sample efficiency.

\section{Methods/Algorithms/Implementations}
We will implement the following list of algorithms for meta-reinforcement learning:
\begin{multicols}{2}
\begin{itemize}
    \item $\text{R}^2$ \cite{duan_rl2_2016}
    \item MAML \cite{finn_model-agnostic_2017}
    \item TMAML \cite{liu_taming_2019}
    \item MACAW \cite{mitchell_offline_nodate}
\end{itemize}
(Optional, if time permits)

\begin{itemize}
    \item PEARL \cite{rakelly_efficient_nodate}
    \item VariBAD \cite{zintgraf_varibad_2020}
    \item BOReL \cite{dorfman_offline_2021}
\end{itemize}
\end{multicols}

The $\text{R}^2$ algorithm is encoded in the weights of an RNN, which are learned through a 
general-purpose "slow" RL algorithm. The RNN receives all information a typical RL algorithm would and retains its state across episodes. The activations of the RNN store the state of the “fast” RL algorithm on the current previously unseen MDP \cite{duan_rl2_2016}. A prior work of \citeauthor{wang_learning_2017} describes a similar context-based algorithm which we will also take a look into.

MAML is a model-agnostic meta-learning algorithm compatible with any model trained using gradient descent. It is applicable to a variety of different learning problems, including classification, regression, and reinforcement learning \cite{finn_model-agnostic_2017}. Building upon that, TMAML proposes a surrogate objective function that adds control variates into the gradient estimation via automatic differentiation \cite{liu_taming_2019}. 

Lastly, to also include an offline-variant, we take a look at MACAW which uses Meta-Actor Critic with Advantage Weighting. It imitates the general approach of pre-training on an existing dataset and then fine-tuning with little data to a new task \cite{mitchell_offline_nodate}.

%PEARL, also an offline meta-RL algorithm, extends this by separating inference and control and uses probabilistic reasoning to take into account uncertainty of new, sparse tasks.


\section{Evaluation}
%Provide related references that you will examine in later stages (if any). Provide evaluation plan of your results (if any). What other algorithms are you going to compare with? How will you evaluate different methods?

To evaluate the suggested algorithms exhaustively, we will make use of a variety of existing benchmark environments. For this, we propose testing the implemented algorithms on simple environments such as Multi-Armed Bandits/Tabular MDPs \cite{duan_rl2_2016} as well as more sophisticated tasks from the OpenAI Gym environment which includes HalfCheetah and Ant \cite{finn_model-agnostic_2017}. Within these, we will mainly compare the gathered cumulative reward and the number of iterations.

\medskip
\nocite{*}
\bibliographystyle{unsrtnat}
\bibliography{ref.bib}


\end{document}
