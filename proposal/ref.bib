@article{wang_learning_2017,
	title = {Learning to reinforcement learn},
	url = {http://arxiv.org/abs/1611.05763},
	abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
	urldate = {2021-10-04},
	author = {Wang, Jane X. and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
	month = jan,
	year = {2017},
	note = {arXiv: 1611.05763},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/rafael/Zotero/storage/SLN9QREE/Wang et al. - 2017 - Learning to reinforcement learn.pdf:application/pdf;arXiv.org Snapshot:/home/rafael/Zotero/storage/8SZQ7LJA/1611.html:text/html},
}

@article{duan_rl2_2016,
	title = {{RL}\${\textasciicircum}2\$: {Fast} {Reinforcement} {Learning} via {Slow} {Reinforcement} {Learning}},
	shorttitle = {{RL}\${\textasciicircum}2\$},
	url = {http://arxiv.org/abs/1611.02779},
	abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL\${\textasciicircum}2\$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL\${\textasciicircum}2\$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL\${\textasciicircum}2\$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL\${\textasciicircum}2\$ on a vision-based navigation task and show that it scales up to high-dimensional problems.},
	urldate = {2021-10-04},
	author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.02779},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/rafael/Zotero/storage/CRBWD8E8/Duan et al. - 2016 - RL\$^2\$ Fast Reinforcement Learning via Slow Reinf.pdf:application/pdf;arXiv.org Snapshot:/home/rafael/Zotero/storage/DABV2VVN/1611.html:text/html},
}

@inproceedings{finn_model-agnostic_2017,
	title = {Model-{Agnostic} {Meta}-{Learning} for {Fast} {Adaptation} of {Deep} {Networks}},
	url = {https://proceedings.mlr.press/v70/finn17a.html},
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1126--1135},
	file = {Full Text PDF:/home/rafael/Zotero/storage/26TYTCK4/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf:application/pdf;Supplementary PDF:/home/rafael/Zotero/storage/TEWRTPIY/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf:application/pdf},
}

@article{mitchell_offline_nodate,
	title = {Offline {Meta}-{Reinforcement} {Learning} with {Advantage} {Weighting}},
	abstract = {This paper introduces the ofﬂine metareinforcement learning (ofﬂine meta-RL) problem setting and proposes an algorithm that performs well in this setting. Ofﬂine meta-RL is analogous to the widely successful supervised learning strategy of pre-training a model on a large batch of ﬁxed, pre-collected data (possibly from various tasks) and ﬁne-tuning the model to a new task with relatively little data. That is, in ofﬂine meta-RL, we meta-train on ﬁxed, pre-collected data from several tasks in order to adapt to a new task with a very small amount (less than 5 trajectories) of data from the new task. By nature of being ofﬂine, algorithms for ofﬂine meta-RL can utilize the largest possible pool of training data available and eliminate potentially unsafe or costly data collection during meta-training. This setting inherits the challenges of ofﬂine RL, but it differs signiﬁcantly because ofﬂine RL does not generally consider a) transfer to new tasks or b) limited data from the test task, both of which we face in ofﬂine meta-RL. Targeting the ofﬂine meta-RL setting, we propose Meta-Actor Critic with Advantage Weighting (MACAW), an optimization-based meta-learning algorithm that uses simple, supervised regression objectives for both the inner and outer loop of meta-training. On ofﬂine variants of common meta-RL benchmarks, we empirically ﬁnd that this approach enables fully ofﬂine meta-reinforcement learning and achieves notable gains over prior methods. Code available at https://sites.google.com/view/macaw-metarl.},
	language = {en},
	year = {2021},
	author = {Mitchell, Eric and Rafailov, Rafael and Peng, Xue Bin and Levine, Sergey and Finn, Chelsea},
	pages = {12},
	file = {Mitchell et al. - Offline Meta-Reinforcement Learning with Advantage.pdf:/home/rafael/Zotero/storage/662VSYKW/Mitchell et al. - Offline Meta-Reinforcement Learning with Advantage.pdf:application/pdf},
}

@article{rakelly_efficient_nodate,
	title = {Efficient {Off}-{Policy} {Meta}-{Reinforcement} {Learning} via  {Probabilistic} {Context} {Variables}},
	abstract = {Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While meta-reinforcement learning (metaRL) algorithms can enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efﬁciency. They also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness on sparse reward problems. In this paper, we address these challenges by developing an offpolicy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic ﬁltering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efﬁcient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both metatraining and adaptation efﬁciency. Our method outperforms prior algorithms in sample efﬁciency by 20-100x as well as in asymptotic performance on several meta-RL benchmarks.},
	language = {en},
	author = {Rakelly, Kate and Zhou, Aurick and Quillen, Deirdre and Finn, Chelsea and Levine, Sergey},
	pages = {10},
	year = {2019},
	file = {Rakelly et al. - Efficient Off-Policy Meta-Reinforcement Learning v.pdf:/home/rafael/Zotero/storage/9UBAL9FH/Rakelly et al. - Efficient Off-Policy Meta-Reinforcement Learning v.pdf:application/pdf},
}

@inproceedings{liu_taming_2019,
	title = {Taming {MAML}: {Efficient} unbiased meta-reinforcement learning},
	shorttitle = {Taming {MAML}},
	url = {https://proceedings.mlr.press/v97/liu19g.html},
	abstract = {While meta reinforcement learning (Meta-RL) methods have achieved remarkable success, obtaining correct and low variance estimates for policy gradients remains a significant challenge. In particular, estimating a large Hessian, poor sample efficiency and unstable training continue to make Meta-RL difficult. We propose a surrogate objective function named, Taming MAML (TMAML), that adds control variates into gradient estimation via automatic differentiation. TMAML improves the quality of gradient estimation by reducing variance without introducing bias. We further propose a version of our method that extends the meta-learning framework to learning the control variates themselves, enabling efficient and scalable learning from a distribution of MDPs. We empirically compare our approach with MAML and other variance-bias trade-off methods including DICE, LVC, and action-dependent control variates. Our approach is easy to implement and outperforms existing methods in terms of the variance and accuracy of gradient estimation, ultimately yielding higher performance across a variety of challenging Meta-RL environments.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Liu, Hao and Socher, Richard and Xiong, Caiming},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {4061--4071},
	file = {Full Text PDF:/home/rafael/Zotero/storage/62BS5BC5/Liu et al. - 2019 - Taming MAML Efficient unbiased meta-reinforcement.pdf:application/pdf;Supplementary PDF:/home/rafael/Zotero/storage/23W2CCGG/Liu et al. - 2019 - Taming MAML Efficient unbiased meta-reinforcement.pdf:application/pdf},
}

@article{zintgraf_varibad_2020,
	title = {{VariBAD}: {A} {Very} {Good} {Method} for {Bayes}-{Adaptive} {Deep} {RL} via {Meta}-{Learning}},
	shorttitle = {{VariBAD}},
	url = {http://arxiv.org/abs/1910.08348},
	abstract = {Trading off exploration and exploitation in an unknown environment is key to maximising expected return during learning. A Bayes-optimal policy, which does so optimally, conditions its actions not only on the environment state but on the agent's uncertainty about the environment. Computing a Bayes-optimal policy is however intractable for all but the smallest tasks. In this paper, we introduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference in an unknown environment, and incorporate task uncertainty directly during action selection. In a grid-world domain, we illustrate how variBAD performs structured online exploration as a function of task uncertainty. We further evaluate variBAD on MuJoCo domains widely used in meta-RL and show that it achieves higher online return than existing methods.},
	urldate = {2021-10-06},
	author = {Zintgraf, Luisa and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
	month = feb,
	year = {2020},
	note = {arXiv: 1910.08348},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/rafael/Zotero/storage/LQJHQR4Z/Zintgraf et al. - 2020 - VariBAD A Very Good Method for Bayes-Adaptive Dee.pdf:application/pdf;arXiv.org Snapshot:/home/rafael/Zotero/storage/3UK7LCMT/1910.html:text/html},
}

@article{dorfman_offline_2021,
	title = {Offline {Meta} {Learning} of {Exploration}},
	url = {http://arxiv.org/abs/2008.02598},
	abstract = {Consider the following instance of the Offline Meta Reinforcement Learning (OMRL) problem: given the complete training logs of \$N\$ conventional RL agents, trained on \$N\$ different tasks, design a meta-agent that can quickly maximize reward in a new, unseen task from the same task distribution. In particular, while each conventional RL agent explored and exploited its own different task, the meta-agent must identify regularities in the data that lead to effective exploration/exploitation in the unseen task. Here, we take a Bayesian RL (BRL) view, and seek to learn a Bayes-optimal policy from the offline data. Building on the recent VariBAD BRL approach, we develop an off-policy BRL method that learns to plan an exploration strategy based on an adaptive neural belief estimate. However, learning to infer such a belief from offline data brings a new identifiability issue we term MDP ambiguity. We characterize the problem, and suggest resolutions via data collection and modification procedures. Finally, we evaluate our framework on a diverse set of domains, including difficult sparse reward tasks, and demonstrate learning of effective exploration behavior that is qualitatively different from the exploration used by any RL agent in the data.},
	urldate = {2021-10-06},
	author = {Dorfman, Ron and Shenfeld, Idan and Tamar, Aviv},
	month = feb,
	year = {2021},
	note = {arXiv: 2008.02598},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/rafael/Zotero/storage/ZL9WC6JF/Dorfman et al. - 2021 - Offline Meta Learning of Exploration.pdf:application/pdf;arXiv.org Snapshot:/home/rafael/Zotero/storage/ZDBRXXZP/2008.html:text/html},
}